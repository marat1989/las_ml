{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kgl 1</th>\n",
       "      <th>kgl 2</th>\n",
       "      <th>kgl 3</th>\n",
       "      <th>kgl 4</th>\n",
       "      <th>kgl 5</th>\n",
       "      <th>kgl 6</th>\n",
       "      <th>kgl 7</th>\n",
       "      <th>kgl 8</th>\n",
       "      <th>kgl 9</th>\n",
       "      <th>kgl 10</th>\n",
       "      <th>...</th>\n",
       "      <th>kgl 92</th>\n",
       "      <th>kgl 93</th>\n",
       "      <th>kgl 94</th>\n",
       "      <th>kgl 95</th>\n",
       "      <th>kgl 96</th>\n",
       "      <th>kgl 97</th>\n",
       "      <th>kgl 98</th>\n",
       "      <th>kgl 99</th>\n",
       "      <th>kgl 100</th>\n",
       "      <th>wc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.227183</td>\n",
       "      <td>0.229219</td>\n",
       "      <td>0.226675</td>\n",
       "      <td>0.216163</td>\n",
       "      <td>0.123856</td>\n",
       "      <td>0.122162</td>\n",
       "      <td>0.124524</td>\n",
       "      <td>0.121009</td>\n",
       "      <td>0.115269</td>\n",
       "      <td>0.122897</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100685</td>\n",
       "      <td>0.107715</td>\n",
       "      <td>0.117733</td>\n",
       "      <td>0.119287</td>\n",
       "      <td>0.108693</td>\n",
       "      <td>0.114867</td>\n",
       "      <td>0.147499</td>\n",
       "      <td>0.131292</td>\n",
       "      <td>0.114781</td>\n",
       "      <td>0.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.396314</td>\n",
       "      <td>0.396314</td>\n",
       "      <td>0.404986</td>\n",
       "      <td>0.305105</td>\n",
       "      <td>0.268331</td>\n",
       "      <td>0.292826</td>\n",
       "      <td>0.262488</td>\n",
       "      <td>0.282108</td>\n",
       "      <td>0.299649</td>\n",
       "      <td>0.277263</td>\n",
       "      <td>...</td>\n",
       "      <td>0.134884</td>\n",
       "      <td>0.121387</td>\n",
       "      <td>0.094553</td>\n",
       "      <td>0.120820</td>\n",
       "      <td>0.347750</td>\n",
       "      <td>0.364641</td>\n",
       "      <td>0.353523</td>\n",
       "      <td>0.336004</td>\n",
       "      <td>0.416229</td>\n",
       "      <td>0.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.293553</td>\n",
       "      <td>0.305314</td>\n",
       "      <td>0.303002</td>\n",
       "      <td>0.308562</td>\n",
       "      <td>0.310217</td>\n",
       "      <td>0.304514</td>\n",
       "      <td>0.310325</td>\n",
       "      <td>0.327548</td>\n",
       "      <td>0.289342</td>\n",
       "      <td>0.289983</td>\n",
       "      <td>...</td>\n",
       "      <td>0.421735</td>\n",
       "      <td>0.435352</td>\n",
       "      <td>0.437670</td>\n",
       "      <td>0.434946</td>\n",
       "      <td>0.434682</td>\n",
       "      <td>0.432638</td>\n",
       "      <td>0.416016</td>\n",
       "      <td>0.415558</td>\n",
       "      <td>0.420287</td>\n",
       "      <td>0.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.377523</td>\n",
       "      <td>0.377523</td>\n",
       "      <td>0.354627</td>\n",
       "      <td>0.357531</td>\n",
       "      <td>0.360867</td>\n",
       "      <td>0.344058</td>\n",
       "      <td>0.333473</td>\n",
       "      <td>0.340105</td>\n",
       "      <td>0.348635</td>\n",
       "      <td>0.345380</td>\n",
       "      <td>...</td>\n",
       "      <td>0.257030</td>\n",
       "      <td>0.288878</td>\n",
       "      <td>0.292152</td>\n",
       "      <td>0.292536</td>\n",
       "      <td>0.296641</td>\n",
       "      <td>0.307266</td>\n",
       "      <td>0.287952</td>\n",
       "      <td>0.315756</td>\n",
       "      <td>0.313107</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.147989</td>\n",
       "      <td>0.179315</td>\n",
       "      <td>0.208930</td>\n",
       "      <td>0.222672</td>\n",
       "      <td>0.197656</td>\n",
       "      <td>0.182883</td>\n",
       "      <td>0.177504</td>\n",
       "      <td>0.179439</td>\n",
       "      <td>0.207470</td>\n",
       "      <td>0.203137</td>\n",
       "      <td>...</td>\n",
       "      <td>0.224901</td>\n",
       "      <td>0.226155</td>\n",
       "      <td>0.237325</td>\n",
       "      <td>0.161780</td>\n",
       "      <td>0.141492</td>\n",
       "      <td>0.140836</td>\n",
       "      <td>0.125245</td>\n",
       "      <td>0.129295</td>\n",
       "      <td>0.129360</td>\n",
       "      <td>0.39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      kgl 1     kgl 2     kgl 3     kgl 4     kgl 5     kgl 6     kgl 7  \\\n",
       "0  0.227183  0.229219  0.226675  0.216163  0.123856  0.122162  0.124524   \n",
       "1  0.396314  0.396314  0.404986  0.305105  0.268331  0.292826  0.262488   \n",
       "2  0.293553  0.305314  0.303002  0.308562  0.310217  0.304514  0.310325   \n",
       "3  0.377523  0.377523  0.354627  0.357531  0.360867  0.344058  0.333473   \n",
       "4  0.147989  0.179315  0.208930  0.222672  0.197656  0.182883  0.177504   \n",
       "\n",
       "      kgl 8     kgl 9    kgl 10  ...     kgl 92    kgl 93    kgl 94    kgl 95  \\\n",
       "0  0.121009  0.115269  0.122897  ...   0.100685  0.107715  0.117733  0.119287   \n",
       "1  0.282108  0.299649  0.277263  ...   0.134884  0.121387  0.094553  0.120820   \n",
       "2  0.327548  0.289342  0.289983  ...   0.421735  0.435352  0.437670  0.434946   \n",
       "3  0.340105  0.348635  0.345380  ...   0.257030  0.288878  0.292152  0.292536   \n",
       "4  0.179439  0.207470  0.203137  ...   0.224901  0.226155  0.237325  0.161780   \n",
       "\n",
       "     kgl 96    kgl 97    kgl 98    kgl 99   kgl 100    wc  \n",
       "0  0.108693  0.114867  0.147499  0.131292  0.114781  0.22  \n",
       "1  0.347750  0.364641  0.353523  0.336004  0.416229  0.39  \n",
       "2  0.434682  0.432638  0.416016  0.415558  0.420287  0.38  \n",
       "3  0.296641  0.307266  0.287952  0.315756  0.313107  0.31  \n",
       "4  0.141492  0.140836  0.125245  0.129295  0.129360  0.39  \n",
       "\n",
       "[5 rows x 101 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "fake_las_kgl = pd.read_csv('fake_data/fake_data_collect_kgl_1.csv', delimiter=';')\n",
    "fake_las_kgl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5322, 101)\n",
      "(2822, 101)\n"
     ]
    }
   ],
   "source": [
    "print(fake_las_kgl.shape)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(10)\n",
    "\n",
    "remove_n = 2500\n",
    "drop_indices = np.random.choice(fake_las_kgl.index, remove_n, replace=False)\n",
    "fake_las_kgl = fake_las_kgl.drop(drop_indices)\n",
    "\n",
    "print(fake_las_kgl.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wc</th>\n",
       "      <th>aps 1</th>\n",
       "      <th>aps 2</th>\n",
       "      <th>aps 3</th>\n",
       "      <th>aps 4</th>\n",
       "      <th>aps 5</th>\n",
       "      <th>aps 6</th>\n",
       "      <th>aps 7</th>\n",
       "      <th>aps 8</th>\n",
       "      <th>aps 9</th>\n",
       "      <th>...</th>\n",
       "      <th>aps 91</th>\n",
       "      <th>aps 92</th>\n",
       "      <th>aps 93</th>\n",
       "      <th>aps 94</th>\n",
       "      <th>aps 95</th>\n",
       "      <th>aps 96</th>\n",
       "      <th>aps 97</th>\n",
       "      <th>aps 98</th>\n",
       "      <th>aps 99</th>\n",
       "      <th>aps 100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.36</td>\n",
       "      <td>0.021171</td>\n",
       "      <td>0.167164</td>\n",
       "      <td>0.168517</td>\n",
       "      <td>0.180933</td>\n",
       "      <td>0.270358</td>\n",
       "      <td>0.309272</td>\n",
       "      <td>0.271265</td>\n",
       "      <td>0.183740</td>\n",
       "      <td>0.233874</td>\n",
       "      <td>...</td>\n",
       "      <td>0.695610</td>\n",
       "      <td>0.692831</td>\n",
       "      <td>0.694262</td>\n",
       "      <td>0.694691</td>\n",
       "      <td>0.681412</td>\n",
       "      <td>0.689085</td>\n",
       "      <td>0.693916</td>\n",
       "      <td>0.735912</td>\n",
       "      <td>0.730630</td>\n",
       "      <td>0.722199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.30</td>\n",
       "      <td>0.767657</td>\n",
       "      <td>0.778258</td>\n",
       "      <td>0.802267</td>\n",
       "      <td>0.794221</td>\n",
       "      <td>0.804585</td>\n",
       "      <td>0.798034</td>\n",
       "      <td>0.746940</td>\n",
       "      <td>0.638812</td>\n",
       "      <td>0.550778</td>\n",
       "      <td>...</td>\n",
       "      <td>0.487127</td>\n",
       "      <td>0.520048</td>\n",
       "      <td>0.475306</td>\n",
       "      <td>0.459142</td>\n",
       "      <td>0.474802</td>\n",
       "      <td>0.416613</td>\n",
       "      <td>0.453713</td>\n",
       "      <td>0.546067</td>\n",
       "      <td>0.538811</td>\n",
       "      <td>0.533616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.27</td>\n",
       "      <td>0.176442</td>\n",
       "      <td>0.279797</td>\n",
       "      <td>0.371725</td>\n",
       "      <td>0.568385</td>\n",
       "      <td>0.653522</td>\n",
       "      <td>0.676041</td>\n",
       "      <td>0.657906</td>\n",
       "      <td>0.657804</td>\n",
       "      <td>0.652358</td>\n",
       "      <td>...</td>\n",
       "      <td>0.787421</td>\n",
       "      <td>0.779257</td>\n",
       "      <td>0.823256</td>\n",
       "      <td>0.881989</td>\n",
       "      <td>0.663973</td>\n",
       "      <td>0.535756</td>\n",
       "      <td>0.554081</td>\n",
       "      <td>0.569215</td>\n",
       "      <td>0.204267</td>\n",
       "      <td>0.225243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.23</td>\n",
       "      <td>0.170853</td>\n",
       "      <td>0.174758</td>\n",
       "      <td>0.172925</td>\n",
       "      <td>0.160756</td>\n",
       "      <td>0.172081</td>\n",
       "      <td>0.209677</td>\n",
       "      <td>0.255528</td>\n",
       "      <td>0.220558</td>\n",
       "      <td>0.168179</td>\n",
       "      <td>...</td>\n",
       "      <td>0.114452</td>\n",
       "      <td>0.146199</td>\n",
       "      <td>0.174419</td>\n",
       "      <td>0.204291</td>\n",
       "      <td>0.161430</td>\n",
       "      <td>0.163495</td>\n",
       "      <td>0.131638</td>\n",
       "      <td>0.138834</td>\n",
       "      <td>0.120472</td>\n",
       "      <td>0.149137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.26</td>\n",
       "      <td>0.543554</td>\n",
       "      <td>0.547731</td>\n",
       "      <td>0.540833</td>\n",
       "      <td>0.543317</td>\n",
       "      <td>0.573796</td>\n",
       "      <td>0.587379</td>\n",
       "      <td>0.566171</td>\n",
       "      <td>0.573379</td>\n",
       "      <td>0.564772</td>\n",
       "      <td>...</td>\n",
       "      <td>0.283059</td>\n",
       "      <td>0.197666</td>\n",
       "      <td>0.179452</td>\n",
       "      <td>0.215053</td>\n",
       "      <td>0.215144</td>\n",
       "      <td>0.164982</td>\n",
       "      <td>0.129092</td>\n",
       "      <td>0.126563</td>\n",
       "      <td>0.159112</td>\n",
       "      <td>0.183826</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     wc     aps 1     aps 2     aps 3     aps 4     aps 5     aps 6     aps 7  \\\n",
       "0  0.36  0.021171  0.167164  0.168517  0.180933  0.270358  0.309272  0.271265   \n",
       "1  0.30  0.767657  0.778258  0.802267  0.794221  0.804585  0.798034  0.746940   \n",
       "2  0.27  0.176442  0.279797  0.371725  0.568385  0.653522  0.676041  0.657906   \n",
       "3  0.23  0.170853  0.174758  0.172925  0.160756  0.172081  0.209677  0.255528   \n",
       "4  0.26  0.543554  0.547731  0.540833  0.543317  0.573796  0.587379  0.566171   \n",
       "\n",
       "      aps 8     aps 9    ...       aps 91    aps 92    aps 93    aps 94  \\\n",
       "0  0.183740  0.233874    ...     0.695610  0.692831  0.694262  0.694691   \n",
       "1  0.638812  0.550778    ...     0.487127  0.520048  0.475306  0.459142   \n",
       "2  0.657804  0.652358    ...     0.787421  0.779257  0.823256  0.881989   \n",
       "3  0.220558  0.168179    ...     0.114452  0.146199  0.174419  0.204291   \n",
       "4  0.573379  0.564772    ...     0.283059  0.197666  0.179452  0.215053   \n",
       "\n",
       "     aps 95    aps 96    aps 97    aps 98    aps 99   aps 100  \n",
       "0  0.681412  0.689085  0.693916  0.735912  0.730630  0.722199  \n",
       "1  0.474802  0.416613  0.453713  0.546067  0.538811  0.533616  \n",
       "2  0.663973  0.535756  0.554081  0.569215  0.204267  0.225243  \n",
       "3  0.161430  0.163495  0.131638  0.138834  0.120472  0.149137  \n",
       "4  0.215144  0.164982  0.129092  0.126563  0.159112  0.183826  \n",
       "\n",
       "[5 rows x 101 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_las_aps = pd.read_csv('fake_data/fake_data_collect_aps_1.csv', delimiter=';')\n",
    "fake_las_aps.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5322, 101)\n",
      "(2822, 101)\n"
     ]
    }
   ],
   "source": [
    "print(fake_las_aps.shape)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(10)\n",
    "\n",
    "remove_n = 2500\n",
    "drop_indices = np.random.choice(fake_las_aps.index, remove_n, replace=False)\n",
    "fake_las_aps = fake_las_aps.drop(drop_indices)\n",
    "\n",
    "print(fake_las_aps.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fake_aps_kgl = pd.merge(fake_las_aps, fake_las_kgl, on=['wc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(334188, 201)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_aps_kgl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(334188,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_values = fake_aps_kgl['wc']\n",
    "y_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(334188, 200)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del fake_aps_kgl['wc']\n",
    "fake_aps_kgl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(334188, 200)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "x_values = np.array(fake_aps_kgl)\n",
    "print(x_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "x_train, x_hold, y_train, y_hold = train_test_split(x_values, y_values, test_size=0.5, random_state=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13\n",
      "0.61\n",
      "0.48\n"
     ]
    }
   ],
   "source": [
    "print(min(y_values))\n",
    "print(max(y_values))\n",
    "print(max(y_values) - min(y_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_24 (Dense)             (None, 250)               50250     \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 200)               50200     \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 49)                4949      \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 1)                 50        \n",
      "=================================================================\n",
      "Total params: 135,649\n",
      "Trainable params: 135,649\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import boston_housing\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "# Устанавливаем seed для повторяемости результатов\n",
    "np.random.seed(42)\n",
    "# Создаем последовательную модель\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(250, activation='relu', input_shape=(x_train.shape[1],)))\n",
    "model.add(Dense(200, activation='relu', input_shape=(250,)))\n",
    "model.add(Dense(100, activation='relu', input_shape=(200,)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(100, activation='relu', input_shape=(100,)))\n",
    "model.add(Dense(49, activation='relu', input_shape=(100,)))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Компилируем модель\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\", metrics=['mae'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\M1\\Anaconda3\\lib\\site-packages\\keras\\models.py:874: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "167094/167094 [==============================] - 89s 530us/step - loss: 0.0011 - mean_absolute_error: 0.0235\n",
      "Epoch 2/60\n",
      "167094/167094 [==============================] - 90s 538us/step - loss: 4.9733e-04 - mean_absolute_error: 0.0159\n",
      "Epoch 3/60\n",
      "167094/167094 [==============================] - 90s 540us/step - loss: 3.2661e-04 - mean_absolute_error: 0.01272s - loss: \n",
      "Epoch 4/60\n",
      "167094/167094 [==============================] - 90s 539us/step - loss: 2.4716e-04 - mean_absolute_error: 0.0110\n",
      "Epoch 5/60\n",
      "167094/167094 [==============================] - 92s 551us/step - loss: 1.9584e-04 - mean_absolute_error: 0.0098\n",
      "Epoch 6/60\n",
      "167094/167094 [==============================] - 92s 552us/step - loss: 1.6468e-04 - mean_absolute_error: 0.0090\n",
      "Epoch 7/60\n",
      "167094/167094 [==============================] - 93s 554us/step - loss: 1.3797e-04 - mean_absolute_error: 0.0083\n",
      "Epoch 8/60\n",
      "167094/167094 [==============================] - 93s 554us/step - loss: 1.2286e-04 - mean_absolute_error: 0.0077\n",
      "Epoch 9/60\n",
      "167094/167094 [==============================] - 93s 557us/step - loss: 1.1754e-04 - mean_absolute_error: 0.0075\n",
      "Epoch 10/60\n",
      "167094/167094 [==============================] - 94s 565us/step - loss: 1.0320e-04 - mean_absolute_error: 0.0071\n",
      "Epoch 11/60\n",
      "167094/167094 [==============================] - 94s 560us/step - loss: 9.4996e-05 - mean_absolute_error: 0.0068\n",
      "Epoch 12/60\n",
      "167094/167094 [==============================] - 93s 558us/step - loss: 9.5456e-05 - mean_absolute_error: 0.0067\n",
      "Epoch 13/60\n",
      "167094/167094 [==============================] - 93s 558us/step - loss: 9.0227e-05 - mean_absolute_error: 0.0065\n",
      "Epoch 14/60\n",
      "167094/167094 [==============================] - 94s 560us/step - loss: 8.7450e-05 - mean_absolute_error: 0.0064\n",
      "Epoch 15/60\n",
      "167094/167094 [==============================] - 93s 559us/step - loss: 7.7856e-05 - mean_absolute_error: 0.0061\n",
      "Epoch 16/60\n",
      "167094/167094 [==============================] - 94s 560us/step - loss: 7.7296e-05 - mean_absolute_error: 0.0060\n",
      "Epoch 17/60\n",
      "167094/167094 [==============================] - 94s 563us/step - loss: 7.5194e-05 - mean_absolute_error: 0.0060\n",
      "Epoch 18/60\n",
      "167094/167094 [==============================] - 94s 560us/step - loss: 7.3610e-05 - mean_absolute_error: 0.0059\n",
      "Epoch 19/60\n",
      "167094/167094 [==============================] - 94s 562us/step - loss: 7.5335e-05 - mean_absolute_error: 0.0059\n",
      "Epoch 20/60\n",
      "167094/167094 [==============================] - 95s 566us/step - loss: 6.8795e-05 - mean_absolute_error: 0.0057\n",
      "Epoch 21/60\n",
      "167094/167094 [==============================] - 95s 566us/step - loss: 6.8006e-05 - mean_absolute_error: 0.0056\n",
      "Epoch 22/60\n",
      "167094/167094 [==============================] - 94s 565us/step - loss: 6.7909e-05 - mean_absolute_error: 0.0056\n",
      "Epoch 23/60\n",
      "167094/167094 [==============================] - 95s 566us/step - loss: 6.5295e-05 - mean_absolute_error: 0.0056\n",
      "Epoch 24/60\n",
      "167094/167094 [==============================] - 95s 569us/step - loss: 6.4741e-05 - mean_absolute_error: 0.0055\n",
      "Epoch 25/60\n",
      "167094/167094 [==============================] - 95s 567us/step - loss: 6.7363e-05 - mean_absolute_error: 0.0055\n",
      "Epoch 26/60\n",
      "167094/167094 [==============================] - 95s 570us/step - loss: 6.8530e-05 - mean_absolute_error: 0.0055\n",
      "Epoch 27/60\n",
      "167094/167094 [==============================] - 95s 570us/step - loss: 6.7841e-05 - mean_absolute_error: 0.0056\n",
      "Epoch 28/60\n",
      "167094/167094 [==============================] - 95s 571us/step - loss: 6.4838e-05 - mean_absolute_error: 0.0054\n",
      "Epoch 29/60\n",
      "167094/167094 [==============================] - 96s 574us/step - loss: 6.3498e-05 - mean_absolute_error: 0.0053\n",
      "Epoch 30/60\n",
      "167094/167094 [==============================] - 96s 577us/step - loss: 6.2618e-05 - mean_absolute_error: 0.0054\n",
      "Epoch 31/60\n",
      "167094/167094 [==============================] - 96s 575us/step - loss: 6.5093e-05 - mean_absolute_error: 0.0054\n",
      "Epoch 32/60\n",
      "167094/167094 [==============================] - 96s 576us/step - loss: 6.0962e-05 - mean_absolute_error: 0.0053\n",
      "Epoch 33/60\n",
      "167094/167094 [==============================] - 97s 579us/step - loss: 6.4370e-05 - mean_absolute_error: 0.0054\n",
      "Epoch 34/60\n",
      "167094/167094 [==============================] - 96s 577us/step - loss: 6.0962e-05 - mean_absolute_error: 0.0053\n",
      "Epoch 35/60\n",
      "167094/167094 [==============================] - 97s 582us/step - loss: 5.9942e-05 - mean_absolute_error: 0.0052\n",
      "Epoch 36/60\n",
      "167094/167094 [==============================] - 97s 581us/step - loss: 5.9842e-05 - mean_absolute_error: 0.0053\n",
      "Epoch 37/60\n",
      "167094/167094 [==============================] - 98s 584us/step - loss: 6.4832e-05 - mean_absolute_error: 0.0054\n",
      "Epoch 38/60\n",
      "167094/167094 [==============================] - 97s 581us/step - loss: 6.1292e-05 - mean_absolute_error: 0.0053\n",
      "Epoch 39/60\n",
      "167094/167094 [==============================] - 98s 588us/step - loss: 6.0567e-05 - mean_absolute_error: 0.0052\n",
      "Epoch 40/60\n",
      "167094/167094 [==============================] - 97s 582us/step - loss: 6.0051e-05 - mean_absolute_error: 0.0052\n",
      "Epoch 41/60\n",
      "167094/167094 [==============================] - 98s 586us/step - loss: 6.1386e-05 - mean_absolute_error: 0.00520s - loss: 6.1417e-05 - mean_absolute_error: 0\n",
      "Epoch 42/60\n",
      "167094/167094 [==============================] - 98s 586us/step - loss: 6.0719e-05 - mean_absolute_error: 0.0052\n",
      "Epoch 43/60\n",
      "167094/167094 [==============================] - 99s 590us/step - loss: 6.0086e-05 - mean_absolute_error: 0.0052\n",
      "Epoch 44/60\n",
      "167094/167094 [==============================] - 98s 587us/step - loss: 5.7611e-05 - mean_absolute_error: 0.0051\n",
      "Epoch 45/60\n",
      "167094/167094 [==============================] - 99s 595us/step - loss: 5.8153e-05 - mean_absolute_error: 0.00511s - loss: 5.7989e-\n",
      "Epoch 46/60\n",
      "167094/167094 [==============================] - 99s 591us/step - loss: 6.1355e-05 - mean_absolute_error: 0.0051\n",
      "Epoch 47/60\n",
      "167094/167094 [==============================] - 99s 593us/step - loss: 5.8432e-05 - mean_absolute_error: 0.0051\n",
      "Epoch 48/60\n",
      "167094/167094 [==============================] - 101s 605us/step - loss: 6.2034e-05 - mean_absolute_error: 0.0052\n",
      "Epoch 49/60\n",
      "167094/167094 [==============================] - 100s 598us/step - loss: 5.9679e-05 - mean_absolute_error: 0.0052\n",
      "Epoch 50/60\n",
      "167094/167094 [==============================] - 99s 594us/step - loss: 6.2314e-05 - mean_absolute_error: 0.0053\n",
      "Epoch 51/60\n",
      "167094/167094 [==============================] - 102s 609us/step - loss: 5.9833e-05 - mean_absolute_error: 0.0051\n",
      "Epoch 52/60\n",
      "167094/167094 [==============================] - 101s 606us/step - loss: 6.0577e-05 - mean_absolute_error: 0.0052\n",
      "Epoch 53/60\n",
      "167094/167094 [==============================] - 102s 611us/step - loss: 5.9467e-05 - mean_absolute_error: 0.0051\n",
      "Epoch 54/60\n",
      "167094/167094 [==============================] - 102s 609us/step - loss: 6.0338e-05 - mean_absolute_error: 0.0051\n",
      "Epoch 55/60\n",
      "167094/167094 [==============================] - 102s 613us/step - loss: 6.3731e-05 - mean_absolute_error: 0.0052\n",
      "Epoch 56/60\n",
      "167094/167094 [==============================] - 102s 611us/step - loss: 5.8352e-05 - mean_absolute_error: 0.0051\n",
      "Epoch 57/60\n",
      "167094/167094 [==============================] - 103s 616us/step - loss: 6.1023e-05 - mean_absolute_error: 0.0052\n",
      "Epoch 58/60\n",
      "167094/167094 [==============================] - 102s 608us/step - loss: 6.1406e-05 - mean_absolute_error: 0.0052\n",
      "Epoch 59/60\n",
      "167094/167094 [==============================] - 103s 615us/step - loss: 6.1090e-05 - mean_absolute_error: 0.0052\n",
      "Epoch 60/60\n",
      "167094/167094 [==============================] - 103s 614us/step - loss: 5.6847e-05 - mean_absolute_error: 0.0051\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x5c877080>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Обучаем сеть\n",
    "model.fit(x_train, y_train, batch_size=5, nb_epoch=60, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_predict = model.predict(x_hold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, explained_variance_score\n",
    "\n",
    "def data_analize_func(predict_for_analize, file_name = ''):\n",
    "    # подготовка данных\n",
    "    data_for_analize = predict_for_analize.sort_values(by=['y_hold'])\n",
    "    data_for_analize['x_axis'] = [x for x in range(len(predict_for_analize['y_hold']))]\n",
    "    data_for_analize.head()\n",
    "    if file_name != '':\n",
    "        data_for_analize.to_csv('AllGisParams/' + file_name, index=False, sep = ';')\n",
    "    \n",
    "    # построение графиков\n",
    "    plt.scatter(data_for_analize['x_axis'], data_for_analize['y_predict'], color = 'blue')\n",
    "    plt.scatter(data_for_analize['x_axis'], data_for_analize['y_hold'], color = 'red')\n",
    "    plt.show()\n",
    "    \n",
    "    # regression metrics\n",
    "    print('MSE                      = '+ str(mean_squared_error(data_for_analize['y_hold'], \n",
    "                                                                data_for_analize['y_predict'])))\n",
    "    print('MAE                      = '+ str(mean_absolute_error(data_for_analize['y_hold'], \n",
    "                                                                 data_for_analize['y_predict'])))\n",
    "    print('r2_score                 = '+ str(r2_score(data_for_analize['y_hold'], \n",
    "                                                      data_for_analize['y_predict'])) + '    (best_value is 1)')\n",
    "    print('explained_variance_score = '+ str(explained_variance_score(data_for_analize['y_hold'], \n",
    "                                                                      data_for_analize['y_predict'])) + '    (best_value is 1)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAD8CAYAAACVZ8iyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABEMgAAiGQAABDJAAAgkgEAQNL/A5Mei1//UWaaAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x5b703588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE                      = 2.94364521871e-05\n",
      "MAE                      = 0.00395930630332\n",
      "r2_score                 = 0.987036764818    (best_value is 1)\n",
      "explained_variance_score = 0.988616292688    (best_value is 1)\n"
     ]
    }
   ],
   "source": [
    "predict_for_analize_fake = pd.DataFrame({'y_hold':y_hold, 'y_predict':y_predict[:,0]})\n",
    "data_analize_func(predict_for_analize_fake)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
