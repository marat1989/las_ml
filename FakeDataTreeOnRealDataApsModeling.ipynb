{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import main\n",
    "import lasio\n",
    "model_path = \"..\\\\tasks\\\\task 6\\\\Data\\\\modelling\\\\\"\n",
    "\n",
    "#l = lasio.read(model_path + '105.las')\n",
    "#print(l.curves.keys())\n",
    "main.create_csv_from_las_modeling(model_path,'modeling_aps.csv')pd.datetime.strptime(x, '%Y.%m.%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013-06-28 00:00:00\n"
     ]
    }
   ],
   "source": [
    "x = \"28.06.2013\"\n",
    "x = pd.datetime.strptime(x, '%d.%m.%Y')\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "CParserError",
     "evalue": "Error tokenizing data. C error: Expected 2 fields in line 11, saw 3\n",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCParserError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-a349e97831ce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmain\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdateparse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'%d.%m.%Y'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mperf_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_dir\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'\\\\perforation_wellname.csv'\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdate_parser\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdateparse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\libs\\Anaconda\\3.3\\envs\\aind\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    644\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    645\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 646\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    647\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    648\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\libs\\Anaconda\\3.3\\envs\\aind\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    399\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    400\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 401\u001b[1;33m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    402\u001b[0m     \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    403\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\libs\\Anaconda\\3.3\\envs\\aind\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    937\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'skipfooter not supported for iteration'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    938\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 939\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    940\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    941\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'as_recarray'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\libs\\Anaconda\\3.3\\envs\\aind\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1506\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1507\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1508\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1509\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1510\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader.read (pandas\\parser.c:10415)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._read_low_memory (pandas\\parser.c:10691)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._read_rows (pandas\\parser.c:11437)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._tokenize_rows (pandas\\parser.c:11308)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.raise_parser_error (pandas\\parser.c:27037)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mCParserError\u001b[0m: Error tokenizing data. C error: Expected 2 fields in line 11, saw 3\n"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import main\n",
    "dateparse = lambda x: pd.datetime.strptime(x, '%d.%m.%Y')\n",
    "perf_data = pd.read_csv(main.data_dir + '\\\\perforation_wellname.csv' , delimiter=' ', date_parser=dateparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import main\n",
    "las_data = pd.read_csv(main.data_dir + '\\\\modeling_aps.csv' , delimiter=';')\n",
    "las_data = las_data.rename(columns={'well_name': 'WELL_NAME_UWI'})\n",
    "las_data['WELL_NAME_UWI'] = las_data['WELL_NAME_UWI'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "key_data = pd.read_csv(main.data_dir + '\\\\well_name_key.csv' , delimiter=';')\n",
    "key_data = key_data.rename(columns={'well_name_uwi': 'WELL_NAME_UWI'})\n",
    "key_data = key_data.rename(columns={'well_name': 'WELL_NAME'})\n",
    "key_data.replace('314_', '', regex=True, inplace=True)\n",
    "key_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "las_data = pd.merge(las_data, key_data, on='WELL_NAME_UWI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "las_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "real_top = pd.read_csv('real_data_test\\\\top_df.csv', delimiter=';')\n",
    "real_top = real_top.rename(columns={'WELL_NAME': 'WELL_NAME_UWI'})\n",
    "del real_top['X']\n",
    "del real_top['Y']\n",
    "real_top.head(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "real_bottom = pd.read_csv('real_data_test\\\\bottom_df.csv', delimiter=';')\n",
    "real_bottom = real_bottom.rename(columns={'WELL_NAME': 'WELL_NAME_UWI'})\n",
    "del real_bottom['X']\n",
    "del real_bottom['Y']\n",
    "real_bottom.head(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# данные от организаторов\n",
    "water_content_data = pd.read_csv(main.data_dir + \"\\\\water_content.csv\", delimiter=';')\n",
    "water_content_data = water_content_data.rename(columns={'Скважина': main.keys_dict[main.kid_well]})\n",
    "water_content_data = water_content_data.rename(columns={' Начальная обводненность ': 'WC'})\n",
    "water_content_data = water_content_data.rename(columns={'Дата запуска': 'date_start'})\n",
    "del water_content_data['date_start']\n",
    "water_content_data = water_content_data.rename(columns={'well_name': 'WELL_NAME'})\n",
    "water_content_data.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# данные посчитанные на карте\n",
    "wc_calc_data = pd.read_csv('fake_data\\\\map_wc_test_calc_res.csv', delimiter=';')\n",
    "del wc_calc_data['Y']\n",
    "del wc_calc_data['X']\n",
    "wc_calc_data['WELL_NAME'] = wc_calc_data['WELL_NAME'].astype('str')\n",
    "wc_calc_data = wc_calc_data.drop(wc_calc_data[wc_calc_data['WC'] > 1].index)\n",
    "wc_calc_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(len(las_data['WELL_NAME'].value_counts()))\n",
    "print(len(real_bottom['WELL_NAME_UWI'].value_counts()))\n",
    "print(len(real_top['WELL_NAME_UWI'].value_counts()))\n",
    "print(len(water_content_data['WELL_NAME'].value_counts()))\n",
    "print(len(wc_calc_data['WELL_NAME'].value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#real_data = pd.merge(water_content_data, las_data, on='WELL_NAME')\n",
    "real_data = pd.merge(wc_calc_data, las_data, on='WELL_NAME')\n",
    "real_data = pd.merge(real_data, real_top, on='WELL_NAME_UWI')\n",
    "real_data = pd.merge(real_data, real_bottom, on='WELL_NAME_UWI') \n",
    "real_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "real_data_na = real_data.dropna()\n",
    "print(len(real_data['WELL_NAME'].value_counts()))\n",
    "real_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import lasio\n",
    "import re\n",
    "def load_and_convert_to_interp(dev_path, well_name):\n",
    "    f = open(dev_path + well_name + '.dev', 'r')\n",
    "    well_num = 0\n",
    "    md = []\n",
    "    abs = []\n",
    "    for line in f.readlines():\n",
    "        if well_num > 16:\n",
    "            # list = line.split(' ')\n",
    "            # print(list)\n",
    "            numbers = re.findall(r'[-]?[0-9]+.[0-9]+', line)\n",
    "            md.append(float(numbers[0]))\n",
    "            abs.append(float(numbers[3]))\n",
    "        well_num = well_num +1\n",
    "    f.close()\n",
    "    f_spline = interpolate.interp1d(abs, md, kind = 'slinear', bounds_error=False)\n",
    "    return f_spline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from scipy import interpolate\n",
    "dev_path = \"..\\\\tasks\\\\task 6\\\\data\\\\dev\\\\\"\n",
    "well_name_list = real_data_na['WELL_NAME_UWI'].value_counts().index.tolist()\n",
    "well_name = well_name_list[1]\n",
    "f_spline = load_and_convert_to_interp(dev_path, well_name)\n",
    "print(f_spline(0))\n",
    "print(well_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "real_data_na = real_data_na.rename(columns={'DEPT': 'DEPTH'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from scipy import interpolate\n",
    "well_name_list = real_data_na['WELL_NAME_UWI'].value_counts().index.tolist()\n",
    "x_values = []\n",
    "y_values = []\n",
    "count_val = 50\n",
    "well_count = 0\n",
    "for well_name in well_name_list:\n",
    "    if well_count%20 == 0:\n",
    "        print(well_count, ' of ', len(well_name_list))\n",
    "    data_well =  real_data_na[real_data_na['WELL_NAME_UWI'] == well_name]\n",
    "    f_spline = load_and_convert_to_interp(dev_path, well_name)\n",
    "    bottom = f_spline(data_well['DEPTH_BOTTOM'].tolist()[0])\n",
    "    top = f_spline(data_well['DEPTH_TOP'].tolist()[0])\n",
    "    data_well_by_bound = data_well[(data_well['DEPTH'] >= top) & (data_well['DEPTH'] <= bottom)]\n",
    "    x_arr = data_well_by_bound['DEPTH']\n",
    "    y_arr = data_well_by_bound['APS']\n",
    "    \n",
    "    # print ('length of array depth', len(x_arr))\n",
    "    # print(len(x_arr), len(y_arr))\n",
    "    if len(x_arr) < 10:\n",
    "        continue\n",
    "    f_spline = interpolate.interp1d(x_arr, y_arr, kind = 'slinear')\n",
    "    h_start = data_well_by_bound['DEPTH'].min()\n",
    "    h_end = data_well_by_bound['DEPTH'].max()\n",
    "    # print(h_start, h_end, top, bottom)\n",
    "    h_step = (h_end - h_start)/count_val\n",
    "    x_temp = []\n",
    "    i = 0\n",
    "    while(i < count_val):\n",
    "        x_temp.append(float(f_spline(h_start + i * h_step)))\n",
    "        i = i + 1\n",
    "    x_values.append(x_temp)  \n",
    "    y_values.append(data_well['WC'].tolist()[0])\n",
    "    well_count = well_count + 1\n",
    "print('end fill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, explained_variance_score\n",
    "\n",
    "def data_analize_func(predict_for_analize, file_name = ''):\n",
    "    # подготовка данных\n",
    "    data_for_analize = predict_for_analize.sort_values(by=['y_hold'])\n",
    "    data_for_analize['x_axis'] = [x for x in range(len(predict_for_analize['y_hold']))]\n",
    "    data_for_analize.head()\n",
    "    if file_name != '':\n",
    "        data_for_analize.to_csv('AllGisParams/' + file_name, index=False, sep = ';')\n",
    "    \n",
    "    # построение графиков\n",
    "    plt.scatter(data_for_analize['x_axis'], data_for_analize['y_predict'], color = 'blue')\n",
    "    plt.scatter(data_for_analize['x_axis'], data_for_analize['y_hold'], color = 'red')\n",
    "    plt.show()\n",
    "    \n",
    "    # regression metrics\n",
    "    print('MSE                      = '+ str(mean_squared_error(data_for_analize['y_hold'], \n",
    "                                                                data_for_analize['y_predict'])))\n",
    "    print('MAE                      = '+ str(mean_absolute_error(data_for_analize['y_hold'], \n",
    "                                                                 data_for_analize['y_predict'])))\n",
    "    print('r2_score                 = '+ str(r2_score(data_for_analize['y_hold'], \n",
    "                                                      data_for_analize['y_predict'])) + '    (best_value is 1)')\n",
    "    print('explained_variance_score = '+ str(explained_variance_score(data_for_analize['y_hold'], \n",
    "                                                                      data_for_analize['y_predict'])) + '    (best_value is 1)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "x_train, x_hold, y_train, y_hold = train_test_split(x_values, y_values, test_size=0.3, random_state=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "forest_reg =  RandomForestRegressor(n_estimators=10,\n",
    "                           random_state=17)\n",
    "# oob_score=True,\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "forest_params = { \n",
    "    'max_depth': [1, 10, 20, 40, 60, 100, 200, 500],\n",
    "    'n_estimators': [1, 10, 30, 50, 70, 100, 150, 300],\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "forest_grid = GridSearchCV(forest_reg, forest_params, cv=5, n_jobs=-1, verbose=True)\n",
    "value_fit = forest_grid.fit(x_train, y_train) \n",
    "y_predict = forest_grid.predict(x_hold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "y_pred = forest_grid.predict(x_hold)\n",
    "predict_for_analize_fake = pd.DataFrame({'y_hold':y_hold, 'y_predict':y_pred})\n",
    "# print('Анализ реальных aps с реальными значениями wc')\n",
    "print('Анализ реальных aps с аппроксимированными значениями wc')\n",
    "data_analize_func(predict_for_analize_fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Loading the saved decision tree model pickle\n",
    "decision_tree_pkl_filename = 'fake_data\\\\decision_forest_grid_fake_data_regressor.pkl'\n",
    "decision_tree_model_pkl_load = open(decision_tree_pkl_filename, 'rb')\n",
    "decision_tree_model = pickle.load(decision_tree_model_pkl_load)\n",
    "print(\"Loaded Decision tree model :: \", decision_tree_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "y_pred = decision_tree_model.predict(x_values)\n",
    "predict_for_analize_fake = pd.DataFrame({'y_hold':y_values, 'y_predict':y_pred})\n",
    "# print('Анализ реальных aps с реальными значениями wc')\n",
    "print('Анализ реальных aps с аппроксимированными значениями wc')\n",
    "data_analize_func(predict_for_analize_fake)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}